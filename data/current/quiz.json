{
  "id": "pmle-20260212-vertex-ai",
  "exam_code": "PMLE",
  "domain": "モデルの提供とスケーリング",
  "section": "モデルの提供",
  "topic": "Vertex AI でのモデル提供",
  "difficulty": "medium",
  "question": "Vertex AI Endpoint にカスタムコンテナを使用したモデルをデプロイしましたが、一部の予測リクエストに対して '500 Internal Server Error' が返されます。Google Cloud コンソールの画面上ではエラーの詳細は確認できません。コンテナ内で発生している具体的な Python 例外やスタックトレースを調査するために、Cloud Logging で最初に行うべきアクションはどれですか？",
  "options": [
    "resource.type=\"aiplatform.googleapis.com/Endpoint\" を指定して、コンテナの stdout や stderr ログをフィルタリングする。",
    "resource.type=\"ai_platform_model\" を指定して、モデルのメタデータ更新ログを確認する。",
    "Cloud Storage のモデルアーティファクト保存先にある logs/ ディレクトリ内のテキストファイルを確認する。",
    "Vertex AI Dashboard の「予測レイテンシ」メトリクスを確認し、エラーが発生したインスタンスを特定する。"
  ],
  "correct": 0,
  "explanation": "Vertex AI のオンライン予測では、デプロイされたコンテナの標準出力 (stdout) および標準エラー出力 (stderr) は自動的に Cloud Logging に転送されます。これらのログは `aiplatform.googleapis.com/Endpoint` というリソースタイプで管理されており、コンテナ内でのランタイムエラーや例外を特定するために最も重要な情報源となります。他の選択肢について、`ai_platform_model` は旧世代の AI Platform のリソースであり、Cloud Storage にログが自動出力されるわけではなく、メトリクスはエラーの発生事象は示しますが具体的な原因（スタックトレースなど）を特定するものではありません。"
}
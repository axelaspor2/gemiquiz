{
  "id": "pmle-20260201-",
  "exam_code": "PMLE",
  "domain": "チーム内およびチーム間の連携によるデータとモデルの管理",
  "section": "組織全体のデータの探索と前処理",
  "topic": "異なるタイプのデータの効率的なトレーニング",
  "difficulty": "hard",
  "question": "あなたは、Google Cloud 上で画像データ（Cloud Storage 貯蔵）と数値メタデータ（BigQuery 貯蔵）を組み合わせたマルチモーダルモデルのトレーニングを Vertex AI で実施しています。A100 GPU を使用しているにもかかわらず、トレーニング速度が予想よりも大幅に遅く、Cloud Profiler を確認したところ GPU 利用率が 15〜20% で推移しており、多くの時間が 'Host-side input processing' に費やされていることが判明しました。画像は数百万個の小さな JPEG ファイルとして保存されています。この I/O ボトルネックを解消し、トレーニング効率を最大化するための最も適切なトラブルシューティング手順はどれですか？",
  "options": [
    "画像データを複数のシャードに分割した TFRecord 形式に変換して Cloud Storage に再保存し、tf.data API の interleave と prefetch を使用してデータの読み込みと前処理を並列化する。",
    "画像を BigQuery のメタデータテーブルに BLOB 型として統合し、BigQuery 読み取り API を使用してネットワークレイテンシを最小化することで、GPU へのデータ供給速度を向上させる。",
    "トレーニングジョブのワーカー数を増やして MultiWorkerMirroredStrategy を適用し、各ワーカーが処理する Cloud Storage 上のファイル数を分散させることで、個々のワーカーの I/O 負荷を軽減する。",
    "Cloud Storage のバケットを「マルチリージョン」から「シングルリージョン」に変更し、GPU インスタンスと同じゾーンに配置することで、物理的な距離によるデータ転送の遅延を解消する。"
  ],
  "correct": 0,
  "explanation": "この問題の根本原因は、Cloud Storage 上に数百万もの小さなファイルが存在することによる「メタデータオーバーヘッド」と「I/O ボトルネック」です。小さなファイルを個別にリクエストすると、ファイルごとの HTTP リクエストに伴うオーバーヘッドが蓄積し、GPU にデータを供給する速度が計算速度を下回ってしまいます（CPU 側でのデータ準備がボトルネックになる）。\n\nA が正解な理由：TFRecord 形式は、複数のレコード（画像など）を 1 つの大きなファイルにまとめることで、シーケンシャルな読み込みを可能にし、リクエスト数を劇的に削減します。また、`tf.data.Dataset.interleave` を使用すると、複数のシャードから並列に読み込みを行い、`prefetch` を使用することで GPU が現在のステップを計算している間に次のデータを CPU で準備（オーバーラップ）できるため、GPU の稼搬率が向上します。\n\n不正解の理由：\nB: BigQuery は大規模な画像（バイナリ）データをディープラーニング向けにストリーミングする用途には最適化されておらず、コストとパフォーマンスの両面で劣ります。\nC: 計算リソース（GPU/ワーカー）を増やしても、データの供給元である I/O 部分がボトルネックである限り、各ワーカーの GPU 利用率は低いままであり、根本的な解決になりません。\nD: リージョンの最適化は重要ですが、数百万の小規模ファイルによるプロトコルオーバーヘッドの方が支配的な要因であるため、このケースでは TFRecord 化ほどの改善は見込めません。"
}
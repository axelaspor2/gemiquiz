{
  "id": "pmle-20260131-",
  "exam_code": "PMLE",
  "domain": "ローコード AI ソリューションの構築",
  "section": "AutoML を使用したモデルのトレーニング",
  "topic": "表形式・テキスト・画像・動画データでのカスタムモデルトレーニング",
  "difficulty": "hard",
  "question": "Vertex AI AutoML Tabular を使用して、過去 1 年間の取引ログから不正利用を検知する分類モデルを構築しました。Google Cloud コンソール上の評価メトリクスでは AUC-PR 0.99 と非常に高い精度を示しましたが、デプロイして直近 1 週間のライブデータでテストしたところ、AUC-PR 0.65 まで大幅に低下しました。このエンジニアはデータの分割にデフォルトの「ランダム分割」を使用しており、特徴量に明らかなターゲットリーク（予測時に不明なはずの情報）は含まれていないことを確認済みです。この問題を解決し、実稼働環境に近い正当な評価を得るためのトラブルシューティングとして、最も適切な対処法はどれですか？",
  "options": [
    "データセット内のタイムスタンプ列を「時間列（Time column）」として指定し、トレーニング、検証、テストセットが時系列順に分割されるように設定して再トレーニングを行う。",
    "データセットに「加重列（Weight column）」を追加し、不正利用のインスタンスに高い重みを割り当てることで、クラス不均衡による過学習を抑制する。",
    "「データ分割列（Data split column）」を手動で作成し、各行に 'TRAIN', 'VALIDATE', 'TEST' の値をランダムに割り当てることで、分割の再現性を確保する。",
    "トレーニング予算（ノード時間）を現在の 2 倍に増やし、モデルがより複雑なパターンを学習できるようにして、汎化性能の向上を図る。"
  ],
  "correct": 0,
  "explanation": "時系列的な性質を持つデータ（取引ログなど）に対してデフォルトの「ランダム分割」を適用すると、トレーニングセットに「未来のデータ」が含まれ、テストセットに「過去のデータ」が含まれる可能性があります。これにより、モデルが未来のパターンをカンニングして過去を予測する「ルックアヘッド・バイアス（Look-ahead bias）」というデータリークが発生し、評価結果が極めて楽観的になります。実稼働環境では常に過去のデータから未来を予測するため、この乖離が発生します。Vertex AI AutoML Tabular では、タイムスタンプ列を「時間列」として指定することで、データを時系列順に（古いものをトレーニング、新しいものをテストに）分割でき、この問題を解決できます。他の選択肢について、加重列はクラス不均衡への対応でありリークは解決しません。手動のランダム分割も時系列リークを防げず、予算の増額は根本的なリーク問題の解決にはなりません。"
}
{
  "quiz": {
    "id": "pmle-20260216-",
    "exam_code": "PMLE",
    "domain": "モデルの提供とスケーリング",
    "section": "オンラインモデルサービングのスケーリング",
    "topic": "本番環境でのモデル最適化（レイテンシ、メモリ、スループット）",
    "difficulty": "easy",
    "question": "オンライン予測サービスにおいて、モデルの精度への影響を最小限に抑えつつ、推論時のレイテンシ（応答時間）を短縮し、メモリ使用量を削減するために推奨される手法はどれですか？",
    "options": [
      "モデルの量子化（Quantization）を適用する",
      "推論時のバッチサイズを大きく設定する",
      "トレーニングデータの量を増やして再学習する",
      "モデルの層の数を増やして複雑にする"
    ],
    "correct": 0,
    "explanation": "モデルの量子化は、重みや活性化関数の精度を（例：32ビット浮動小数点から8ビット整数へ）変換することで、モデルサイズを縮小し、計算量を減らして推論を高速化する手法です。これにより、レイテンシとメモリ使用量の両方を改善できます。推論時のバッチサイズを大きくすると、システム全体のスループットは向上しますが、個々のリクエストのレイテンシは増加するため、オンライン予測のレイテンシ改善には適しません。他の選択肢は学習時の精度向上や複雑化に関するものであり、推論の最適化には寄与しません。"
  },
  "message_id": "1472827146006167716",
  "channel_id": "1465902692101787799",
  "posted_at": "2026-02-16T05:29:15.582Z"
}
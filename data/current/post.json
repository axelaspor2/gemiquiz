{
  "quiz": {
    "id": "pde-20260123-apache-beam",
    "exam_code": "PDE",
    "domain": "データの取り込みと処理",
    "section": "データパイプラインの設計",
    "topic": "Apache Beam プログラミングモデル",
    "difficulty": "medium",
    "question": "あるデータエンジニアが、Google Cloud Dataflow を使用して、Cloud Storage からの過去のログ（バッチ）と Pub/Sub からのリアルタイムメッセージ（ストリーミング）の両方を処理するパイプラインを設計しています。Apache Beam プログラミングモデルにおいて、これら性質の異なるデータセットを統一的に表現し、パイプラインの各ステップでの入力と出力として機能する抽象化概念はどれですか？",
    "options": [
      "PCollection",
      "PTransform",
      "Pipeline",
      "Runner"
    ],
    "correct": 0,
    "explanation": "正解は PCollection です。Apache Beam のプログラミングモデルにおいて、PCollection は分散されたデータセットを表現する主要な抽象化概念です。PCollection の最大の特徴は、サイズが固定された「有界（bounded）」データ（バッチ処理）と、無限に続く「無界（unbounded）」データ（ストリーミング処理）の両方を同じプログラミングインターフェースで扱える点にあります。これにより、開発者はデータソースの違い（ファイルかメッセージストリームか）を意識せずに同一のロジックでパイプラインを構築できます。\n\n他の選択肢が不正解の理由：\n・PTransform: データに対する具体的な操作（フィルタリング、変換、結合など）を定義する処理ステップです。PCollection を入力として受け取り、新たな PCollection を出力します。データそのものではなく、処理のロジックを表します。\n・Pipeline: データソースからの読み込み、一連の変換（PTransform）、最終的な書き出しまでのデータ処理ジョブ全体を表現するグラフ（有向非巡回グラフ）です。\n・Runner: 作成されたパイプラインを、Google Cloud Dataflow や Apache Spark、Apache Flink などの特定の分散処理エンジンで実行するための環境（コンポーネント）を指します。"
  },
  "message_id": "1464060387216462020",
  "channel_id": "1298251380527726645",
  "posted_at": "2026-01-23T00:53:17.405Z"
}
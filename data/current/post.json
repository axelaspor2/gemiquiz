{
  "quiz": {
    "id": "pmle-20260131-automl",
    "exam_code": "PMLE",
    "domain": "ローコード AI ソリューションの構築",
    "section": "AutoML を使用したモデルのトレーニング",
    "topic": "AutoML での表形式ワークフロー",
    "difficulty": "hard",
    "question": "Vertex AI の 'Tabular Workflow for End-to-End AutoML' を使用して分類モデルを構築したところ、高精度なアンサンブルモデルが得られましたが、オンライン予測における推論レイテンシがビジネス要件（100ms以内）を超過していることが判明しました。予測精度への影響を最小限に抑えつつ、モデルのサイズを削減し推論速度を向上させるための Google Cloud 推奨のベストプラクティスはどれですか？",
    "options": [
      "モデル蒸留（Distillation）を有効にして、大規模なアンサンブルモデルの出力を模倣するより小規模なモデルをトレーニングする。",
      "アーキテクチャ探索（Architecture Search）をスキップするように設定し、既存のチェックポイントから推論専用の軽量モデルを抽出する。",
      "Feature Transform Engine を使用して主成分分析（PCA）を実行し、モデルに入力する特徴量の次元を圧縮する。",
      "トレーニング予算（Training Budget）を半分に制限することで、生成されるアンサンブル内のモデル個数を強制的に削減する。"
    ],
    "correct": 0,
    "explanation": "Vertex AI の Tabular Workflow for End-to-End AutoML では、'モデル蒸留（Distillation）' という高度な設定が提供されています。これは、精度は高いが計算コストの大きいアンサンブルモデル（教師モデル）の知識を、より単純で高速なモデル（生徒モデル）に継承させる手法です。公式ドキュメントでは、モデルサイズの削減と推論レイテンシの改善のための直接的な手段として蒸留が推奨されています。選択肢Bのアーキテクチャ探索スキップは主に再トレーニング時のコスト削減に寄与するものであり、選択肢Cの PCA は前処理の変更、選択肢Dの予算制限はモデル品質そのものを不安定にする可能性があり、レイテンシ最適化のベストプラクティスとしては蒸留が最も適切です。"
  },
  "message_id": "1467022684671053855",
  "channel_id": "1465902692101787799",
  "posted_at": "2026-01-31T05:04:24.072Z"
}